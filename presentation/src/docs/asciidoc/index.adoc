:customcss: css/custom.css

[state=title]
= CD on Kubernetes that scale
Bjarte Stien Karlsen & Kristoffer Moberg Christensen
2019-09-14
:revnumber: {project-version}

[.image-slide]
== [.underline]#*CRITICAL VULNERABILITY*# out that affects all your images.
image::images/security-bug.jpg[canvas, size=cover]
[.credit]
credit:foobar

[state=red-font]
== Intro
* Bjarte Stien Karlsen, Architect & Developer in the Norwegian Tax Administration(NTA)
* Kristoffer Moberg Christensen, Trainee in NTA

[state=red-font]


== Agenda
* *Background*
* Sprocket
* Use cases
* How can you adopt Sprocket?
* How to expand on Sprocket?
* Plans & Improvements

[state=red-font]
=== Skatteetaten og Kubernetes

=== Background
 * Semantic Version based version strategy
 * BuildConfigs and CustomBuilder for building
 * Scheduled ImageStreams for rolling out new version

== Aurora Version Strategy
plantuml::versionStrategy.puml["versionStrategy", png]

=== Strategies
[#strategies]
|===
|Name | Description
|latest |Any new build
|1 |New features and patches
|1.2 | New patches
|1.2.3 | New infrastructure patches
|1.2.3-b1.8.1-wingnut11-1.3.1| locked
|===

== Build
plantuml::buildConfig.puml["buildConfig1", png]

== BaseImage Change
plantuml::buildConfig2.puml["buildConfig2", png]

== Code change
plantuml::buildConfig.puml["buildConfig3", png]

=== Updating what is running
 * Scheduled ImageStreams

== Features and bugfixes
plantuml::imageStream.puml["imageStream1", png]

== Locked
plantuml::imageStream2.puml["imageStream2", png]

== Infrastructure patches
plantuml::imageStream3.puml["imageStream3", png]

=== Problem #1: Performance
Polling for new changes to lots of images all the time does not scale

 * Vet ikke hvor mye vits det er å prate mye om dette?
 * det kan ta 2 sec å pulle et docker image
 * 1000 som skal sjekkes for
 * 2 min pause mellom hver gang loopen kjøres
 * serial
 * max 120 images per minute
 * hvis alle fyrer og tar 2 sec

MaxScheduledImageImportsPerMinute=120
ScheduledImageImportMinimumIntervalSeconds=120

=== Problem #2: Race condition on multiple triggers
If a Pod has two containers and both have new versions who triggers first?

=== Problem #3: No flow control
Updating the base image/builder will fire every single build at the same time

=== Needs
 * push based, reacting to events/webhooks
 * support Nexus (hosted/grouped repos)
 * enable flow control

=== Can OpenSource help?
 * looked at a lot of alternatives
 * most are based on polling
 * most promising is https://keel.sh/docs/#introduction[keel]
 ** supports WebHook/push based
 ** does not support OpenShift resources
 ** does not support Nexus Container Registry
 ** no flow control

=== What primitives can help us here?
 * notifications from DockerRegistries
 * notifications from build pipelines

=== Build our own
 * No OpenSource solution so we decided to build our own based on notifications from Nexus Container Registry notifications
 * Current solution ties us to OpenShift

== Agenda
* Background
* *Sprocket*
* Use cases
* How can you adopt Sprocket?
* How to expand on Sprocket?

[state=left-box]
== Sprocket
image::images/sprocket.jpg[canvas, size=cover]
[.credit]
credit:https://barkpost.com/cute/the-best-muppet-dogs/

[state=red-font]
== Sprocket MVP
plantuml::sprocket.puml["sprocket", png]

=== Facts
* Currently running as a Pilot in our Platform
* Sprocket is named after the dog in Fraggle Rock

=== Limitations
 * no resilience if event fails
 ** we get an alert from Nexus but nothing automatic
 * only support a single cluster
 * no flow control
 * only support deploy on OpenShift via ImageStream

=== Sequence
 * listen to globalEventHook
 * filter out garbage events
 * parse event into a ImageChangeEvent(s)
 ** one hosted registry can have several groups so it can fire multiple events
 * find related kubernetes resources
 * perform builds/deployments/imports

=== Demo
 * show a video of how Sprocket works
 * building a new image will trigger a deploy on an event

=== Video manus
 - 3 terminaler
 - topp bygg
 - venstre sprocket log
 - høyre: stern for apper

 * start bygg uten sprocket annotasjon
 * vis at det kommer sprocket event men ikke at det rulles ut noe

 * annoter app 1
 * start bygg, vis at det kommer
 * annoter app 2
 * start bygg, vis at begge kommer

=== Permissions
- list ImageStream, perform ImageStreamUpdate
- list, patch Deployment
- list, rollout DeploymentConfig
- list BuildConfig, start Build
- list Sprocket

== Agenda
* Background
* *Sprocket*
* Use cases
* How can you adopt Sprocket?
* How to expand on Sprocket?


=== Use cases
How you can use Sprocket is correlated heavily on how you version your docker images. Does the version change if you change the way you build Docker Images or if the base image changes?

[state=red-font]
=== Our version strategy
https://skatteetaten.github.io/aurora/documentation/openshift/#image-versioning-strategy-the-auroraversion

* example from postgresql dockerhub. Screenshot?
* (Should be elaborated)

=== Plans
 * rate limiting
 ** label-name: skatteetaten.no/sprocket
 ** rate limit for type of event/action. Only run 100 builds then wait 5 sec
 * avoid duplicate rollouts from a single source
 * distributing ImageChangeEvent to multiple clusters
 * audit logging
 * fallback mechanism that can periodically poll and fire ImageChangeEvent
 * cache part of DockerManifest when an ImageChangeEvent happends

=== Expand to other Resources
 - DeploymentConfig
 - Deployment/StatefulSet/DaemonSet
 - Sprocket CRD
 - BuildConfig

[state=red-font]
== Image change triggers
Pushing several tags for the same image allows user to decide when to update the deployment

[state=red-font]
== How can you adopt Sprocket?
* In your build pipeline push to a tag that is updated when there are new versions released on this release track.
* In your Deployments listen to this moveable tag and not an immutable tag
* label your Deployment with an sprocket label that is the hash of the URL to this tag.
* run sprocket configured to listen to Deployments in this namespace(or all namespaces)

[state=red-font]
== How to expand on Sprocket?
* Rate limit changes for one image
* Invalidate manifest cache
* support 1-many triggers via a CRD Sprocket.
* this will enable you to _not_ touch Deployment/ImageStream/DeploymentConfig resources. Only the CRD.
