:customcss: css/custom.css

[state=title]
= CD on Kubernetes that scale
Bjarte Stien Karlsen & Kristoffer Moberg Christensen
2019-09-14
:revnumber: {project-version}

//Kristoffer
[.image-slide]
== [.underline]#*CRITICAL VULNERABILITY*# out that affects all your images.
image::images/security-bug.jpg[canvas, size=cover]
[.credit]
credit:foobar

[.notes]
--
* Stay in this talk to learn how we are building a tool to help us tackle this scenario
* After you leave you will be equiped with more knowledge and maybe a tool that can help you?
--

[state=red-font]
== Intro
* Kristoffer Moberg Christensen, Trainee in the Norwegian Tax Administration(NTA)
* Bjarte Stien Karlsen, Architect & Developer in NTA

== !
[.single-headline2]
Sprocket MVP demo

[.notes]
--
* Start build to simulate that CVE is fixed
* Nothing happends
* Ensure sprocket is running
* Label imageStream
* Start deploy, it is rolled out
--

== Questions?
* Time at the end for questions
* Or come to our stand, we will both be there after the talk

== Agenda
* *Skatteetaten(NTA) and Kubernetes*
* Status quo and problems?
* Needs and requirements
* Sprocket MVP
* Sprocket 1.0

== PaaS in NTA
 * Running OpenShift since 2015
 * Semantic Version based version strategy
 * BuildConfig, CustomBuilder and triggers for building
 * Scheduled ImageStreams for Continuous Delivery (CD)

== Our clusters
* 6 stable-clusters
* 120 nodes
* 5500 pods
* on-prem OpenShift 3.11

[.notes]
--
* to Kristoffer: We have a specific way of versioning docker images
--

== !
image::images/postgres.png[postgres, size=cover]

[.notes]
--
* common pattern
* one instance of a Docker Image has multiple tags
* tags can be overwritten
--

== Postgres strategy
[#strategies]
|===
|Name   | Description
|latest | Any new build
|9      | New features and patches
|9.6    | New patches
|9.6.14 | New when base image changes/builder
|===

[.notes]
--
* briefly talk about the different strategies
--


== !
image::images/postgres_baseimage.png[postgres_baseimage, size=cover]

[.notes]
--
* what base image is the standard one?
* it might be in metadta, but why not as tag?
--

== !
image::images/postgres_baseimage_version.png[postgres_baseversion, size=cover]


[.notes]
--
* alpine classifier singla the base image name, but what about version?
--

== !
image::images/postgres_builder_logic.png[postgres_builder, size=cover]

[.notes]
--
* Where does the builder logic change? Does it? Can i rebuild old image?
* Most applicable if you have centralized builder logic like we do
* transition to Bjarte: So how do we do it?
--

// Bjarte
== Aurora Version
plantuml::versionStrategy.puml["versionStrategy", png]


[.notes]
--
* Extra tag that clearly show alle the parts of a version
* central component of our own CD pipeline
--

==!
[.single-headline2]
Current CD pipeline

== Build
plantuml::buildConfig.puml["buildConfig1", png]

== BaseImage Change
plantuml::buildConfig2.puml["buildConfig2", png]

== Code change
plantuml::buildConfig3.puml["buildConfig3", png]

// Kristoffer
== Update
plantuml::imageStream.puml["imageStream", png]

== New base image
plantuml::imageStream1.puml["imageStream1", png]

== Issue #1: Performance
Polling for new changes to lots of images all the time does not scale. Reported last javaZone.

== Issue #2: No flow control
Updating the base image/builder will fire every single build at the same time

== Issue #3: OpenShift
Current solution ties us to OpenShift

// 10 min
// Headline

// Bjarte
== Needs
 * push based, reacting to events/webhooks
 * support Nexus (hosted/grouped repos)
 * support OpenShift resources
 * enable flow control
 * rate limiting

== Can OpenSource help?
 * looked at a lot of alternatives
 * most are based on polling
 * most promising is https://keel.sh/docs/#introduction[keel]
 ** supports WebHook/push based
 ** does not support OpenShift resources
 ** does not support Nexus Container Registry
 ** no flow control

== What primitives can help us here?
 * notifications from DockerRegistries
 * labels on resources enable efficient queries
 * label values have limitations, so sha1 the content.
 * CRD are possible to complex workflow/configuration

== Build our own
 * No OpenSource solution so we decided to build our own based on notifications from Nexus Container Registry notifications

[state=left-box]
== Sprocket
image::images/sprocket.jpg[canvas, size=cover]
[.credit]
credit:https://barkpost.com/cute/the-best-muppet-dogs/

== Sprocket MVP
plantuml::sprocket-mvp.puml["sprocket-mvp", png]

== Sprocket MVP
* Only supports Nexus Container Registry
* Only works in the cluster you deploy it in with ImageStreams
* You need to build the Docker Image yourself
* It depends on how you build and version images

== Version Strategy
plantuml::versionStrategy.puml["versionStrategy", png]

== !
image::images/postgres.png[postgres, size=cover]

== Installation steps
* Build the sprocket docker image
* configure global event hook in your Nexus Container Registry
* set up and configure sprocket with shared secret from Nexus
* Start sprocket

== Label the resources
* Update your kubernetes manifests to include skatteetaten.no/sprocket labels
* Or update some resources manually to test it out

== !
[.single-headline2]
Sprocket 1.0

== Features
 * persistence of events
 * leader-election for resiliency and scale
 * support multiple clusters/resources
 * flow-control
 * management api
 * optional approval of events
 * support other input
 * hooks (onUpdate, onImageChangeEvent)
 * fallback loop for handling missing events

== Node
plantuml::sprocket-flowcontrol-node.puml["sprocket-flowcontrol-node", png]

== Leader
plantuml::sprocket-flowcontrol-leader.puml["sprocket-flowcontrol-leader", png]

== Resources
plantuml::sprocket-flow.puml["sprocket-flow", png]

== Management
* approve/reject an AffectedImage
* manage ResourceQueue and the related Runners
* manage imageChangeEvent hook
* manage fallback loop

== ResourceFilterer
* An AffectedResource is put into the ResourceQueue unless:
** It is already in the queue waiting to be processed
** It required approval from one or more roles

== ResourceQueue
 * Partitioned on Builds/Deploys for each cluster
 * Will be rate limited according to configuration
 * Each partition can be started/stopped in management api

== Hooks
 * On imageChangeEvent/onUpdate fire a webhook to an endpoint
 ** cloudevents
 * for invalidating manifest cache or tag list cache
 ** nexus does not perform well for fetching tags/manifests

== Fallback loop
* Sometimes events will fail
* Periodically check for outdated resource
** fire a ImageChangeEvent if not up to date

== Support more inputs
 - support events from vanilla Docker Registry
 - support CloudEvents for arbitary input source

== Conclusion
 - Create tools and processes to automate CD
 - Prefer push based model
 - Crate fallback pull based loops for resiliency

== Fin
 - https://github.com/skatteetaten/sprocket
 - https://skatteetaten.github.io/aurora/
 - Come to our stand to talk more!
 - We hope to release Sprocket to a Docker Registry near you later this year.


