:customcss: css/custom.css

[state=title]
= CD on Kubernetes that scale
Bjarte Stien Karlsen & Kristoffer Moberg Christensen
2019-09-14
:revnumber: {project-version}

//Kristoffer
[.image-slide]
== [.underline]#*CRITICAL VULNERABILITY*# out that affects all your images.
image::images/security-bug.jpg[canvas, size=cover]
credit:foobar

[.notes]
--
* Critical vulnerability out
* Affects all images
* React now
* Stay in this talk to learn how we are building a tool to help us tackle this scenario
* After you leave you will be equiped with more knowledge and maybe a tool that can help you?
--

[state=red-font]
== Intro
* Kristoffer Moberg Christensen, Trainee in the Norwegian Tax Administration(NTA)
* Bjarte Stien Karlsen, Architect & Developer in NTA

== !
[.single-headline2]
Sprocket MVP demo

[.notes]
--
* Start build to simulate that CVE is fixed
* Nothing happends
* Ensure sprocket is running
* Label imageStream
* Start deploy, it is rolled out
--

== Questions?
* Time at the end for questions
* Or come to our stand, we will both be there after the talk

== Agenda
* *Skatteetaten(NTA) and Kubernetes*
* Status quo and problems?
* Needs and requirements
* Sprocket MVP
* Sprocket 1.0

== PaaS in NTA
 * Running OpenShift since 2015
 * Semantic Version based version strategy
 * BuildConfig, CustomBuilder and triggers for building
 * Scheduled ImageStreams for Continuous Delivery (CD)

[.notes]
--

--

== Our clusters
* 6 stable-clusters
* 120 nodes
* 5500 pods
* on-prem OpenShift 3.11

[.notes]
--
* transition to Kristoffer: We have a specific way of versioning docker images
--

== !
image::images/postgres.png[postgres, size=cover]

[.notes]
--
* common pattern
* one instance of a Docker Image has multiple tags
* tags can be overwritten
--

== Postgres strategy
[#strategies]
|===
|Name   | Description
|latest | Any new build
|9      | New features and patches
|9.6    | New patches
|9.6.14 | New when base image changes/builder
|===

[.notes]
--
* briefly talk about the different strategies
--


== !
image::images/postgres_baseimage.png[postgres_baseimage, size=cover]

[.notes]
--
* what base image is the standard one?
* it might be in metadata, but why not as tag?
--

== !
image::images/postgres_baseimage_version.png[postgres_baseversion, size=cover]


[.notes]
--
* alpine classifier shows the base image name, but what about version?
--

== !
image::images/postgres_builder_logic.png[postgres_builder, size=cover]

[.notes]
--
* Where does the builder logic change? Does it? Can i rebuild old image?
* Most applicable if you have centralized builder logic like we do
* transition to Bjarte: So how do we do it?
* maybe trainsition: So Bjarte. Enlighten us. Is there a better way?
--

// Bjarte
== Aurora Version
plantuml::versionStrategy.puml["versionStrategy", png]

[.notes]
--
* Extra tag that clearly show all the parts of a version
* central component of our own CD pipeline
--

== !
[.single-headline2]
Current CD pipeline

== Build
plantuml::buildConfig.puml["buildConfig1", png]

== BaseImage Change
plantuml::buildConfig2.puml["buildConfig2", png]

== Code change
plantuml::buildConfig3.puml["buildConfig3", png]

// Kristoffer
== Update
plantuml::imageStream.puml["imageStream", png]

== New base image
plantuml::imageStream1.puml["imageStream1", png]

[.notes]
--
    * Transition to issues: There are some issues with Openshift and ImageStreams
--

== Issue #1: Performance
Polling for new changes to lots of images all the time does not scale. Reported last javaZone.

[.notes]
--
    * Our installation has few and large clusters and does not scale
    * Polling is not optimal
--

== Issue #2: No flow control
Updating the base image/builder will fire every single build at the same time

[.notes]
--
    * The builtin flow control in Kubernetes and Openshift does not meet our needs
    * We want to have more control of how images are built and deployed
    * Currently rebuilding a base image will fire every single build until kubernetes cluster reaches its resource limits
--


== Issue #3: OpenShift
Current solution ties us to OpenShift

[.notes]
--
    * Want 
    * transition to Bjarte:  So Bjarte, what did we do next? 
--

// 10 min
// Headline

// Bjarte
== Needs
 * push based, reacting to events/webhooks
 * support Nexus (hosted/grouped repos)
 * support OpenShift resources
 * enable flow control
 * rate limiting

 [.notes]
 --

 --

== Can OpenSource help?
 * looked at a lot of alternatives
 * most are based on polling
 * most promising is https://keel.sh/docs/#introduction[keel]
 ** supports WebHook/push based
 ** does not support OpenShift resources
 ** does not support Nexus Container Registry
 ** no flow control

== What primitives can help us here?
 * notifications from DockerRegistries
 * labels on resources enable efficient queries
 * label values have limitations, so sha1 the content.
 * CRD are possible to complex workflow/configuration

[.notes]
--
    * (noe sånn)Transition to kristoffer: What did we make of this information? 
--

// Or kristoffer?

== Build our own
 * No OpenSource solution so we decided to build our own based on notifications from Nexus Container Registry notifications

 [.notes]
 --
    Transition to Kristoffer: 
 --

// Kristoffer

[state=left-box]
== Sprocket
image::images/sprocket.jpg[canvas, size=cover]
[.credit]
credit:https://barkpost.com/cute/the-best-muppet-dogs/

[.notes]
--
    * Fraggle rock inspiration
    * 
--
== Sprocket MVP
plantuml::sprocket-mvp.puml["sprocket-mvp", png]


== Sprocket MVP
* Only supports Nexus Container Registry
* Only works in the cluster you deploy it in with ImageStreams
* You need to build the Docker Image yourself
* It depends on how you build and version images

[.notes]
--
    Hva skal vi si her? Står det for mye på slidene? 
--

== Version Strategy
plantuml::versionStrategy.puml["versionStrategy", png]

== !
image::images/postgres.png[postgres, size=cover]

== Installation steps
* Build the sprocket docker image
* configure global event hook in your Nexus Container Registry
* set up and configure sprocket with shared secret from Nexus
* Start sprocket

== Label the resources
* Update your kubernetes manifests to include skatteetaten.no/sprocket labels
* Or update some resources manually to test it out

== !
[.single-headline2]
Sprocket 1.0

== Features
 * persistence of events
 * leader-election for resiliency and scale
 * support multiple clusters/resources
 * flow-control
 * management api
 * optional approval of events
 * support other input
 * hooks (onUpdate, onImageChangeEvent)
 * fallback loop for handling missing events

== Node
plantuml::sprocket-flowcontrol-node.puml["sprocket-flowcontrol-node", png]

== Leader
plantuml::sprocket-flowcontrol-leader.puml["sprocket-flowcontrol-leader", png]

== Resources
plantuml::sprocket-flow.puml["sprocket-flow", png]

== Management
* approve/reject an AffectedImage
* manage ResourceQueue and the related Runners
* manage imageChangeEvent hook
* manage fallback loop

== ResourceFilterer
* An AffectedResource is put into the ResourceQueue unless:
** It is already in the queue waiting to be processed
** It required approval from one or more roles

== ResourceQueue
 * Partitioned on Builds/Deploys for each cluster
 * Will be rate limited according to configuration
 * Each partition can be started/stopped in management api

== Hooks
 * On imageChangeEvent/onUpdate fire a webhook to an endpoint
 ** cloudevents
 * for invalidating manifest cache or tag list cache
 ** nexus does not perform well for fetching tags/manifests

== Fallback loop
* Sometimes events will fail
* Periodically check for outdated resource
** fire a ImageChangeEvent if not up to date

== Support more inputs
 - support events from vanilla Docker Registry
 - support CloudEvents for generic input source

== Conclusion
 - Create tools and processes to automate CD
 - Prefer push based model
 - Crate fallback pull based loops for resiliency

== Fin
 - https://github.com/skatteetaten/sprocket
 - https://skatteetaten.github.io/aurora/
 - Come to our stand to talk more!
 - Have alternate ways of doing this? Please reach out to us and we can talk!
 - We hope to release Sprocket to a Docker Registry near you later this year.


